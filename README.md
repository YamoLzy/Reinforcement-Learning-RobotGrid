# Reinforcement-Learning-RobotGrid
- This project aims to using reinforcement learning (RL) in World Grid Navigation. Its main task is to apply Q-learning process in an robot and make it possible to find a best ‚Äúroad‚Äù from the start point to the target point in a 10 √ó 10 grid map. The robot needs to have the ability to choose the best next state with the help of learned Q values and reward in each trial and try to make the best plan. An important point is that there is no desired target path. Therefore, this is an unsupervised learning problem. In this project, all start points are set to be 1 and ending point are set to be 100. 

1. Firstly, we need initialize all hyper-parameters. They are the discount factor ùõæ, the exploration probability ùúÄ!, and the learning rate ùõº!. They also include the start state, max number in one trial k_max and max number in one run trial_max, i.e. max number of all trials. We also need to initial the most important one, the Q function which is a 100√ó4 matrix.Inthiscomputerexperiment,theinitialQissettobeanall-zeromatrix. Reward function is already given, so it can be used directly by loading it into MATLAB, and its dimension is same as the Q function which is 100 √ó 4. The four columns in reward function and Q function represents the four actions the robot can take in each state, they are move up, move down, move left and move right.

2. After initialization, the first thing need to do is to decide next state which is linked to which action the robot take in present state. We have already known that there are four actions in each state. However, these four actions are not all available for all states. For instance, state one which is the start point cannot choose action one or action four because it will get out of the planned map, and this will cause mistake when run the code. State eleven or state nine have the same dilemma. Hence, we must set a restriction on candidates of actions for these kinds of states which are exactly on the boundary so that the robot will always be in the grid that we set before. To fix this problem, I use a little technique here. We recognized that the rewards for those actions to go beyond boundary are all set as minus one (-1). So when choosing actions for these ‚Äúboundary points‚Äù, we only choose those actions that their rewards are bigger than zero or equal to zero in order to prevent errors later in updating states. Sometimes when applying exploration, there may be not only one actions that could be chosen. In this kind of situation, we uniformly randomly choose one among all candidates.

3. Q-learning is the most important one in the whole reinforcement learning. In this step, we update the Q matrix in each iteration in the for loop. The update will always be ongoing until the learning rate is smaller than 0.005 or the max iteration step is reached, and finally we will get the optimal Q function in the end. The update process will use the reward function defined before. After the loop finished, we can use the Qfinal to decide the optimal policy using Greedy Policy. Finally, we will get a a_optim column vector saving all optimal actions in each state and a s_all column vector saving all states through a particular trajectory.

- We have totally run 10 runs and each run consists of N trials, and the max number of N is 3000. We record the number of goal-reached runs in among 10 runs and record execution time under all situations with different parameters which is in table 1. We also write down the total reward of all cases and plot some trajectory under different situations for an intuitive understanding.

<img width="761" alt="Êà™Â±è2021-06-02 ‰∏ãÂçà12 02 26" src="https://user-images.githubusercontent.com/65494921/120422416-c73d5f00-c39a-11eb-8bca-d246e16bac39.png">

<img width="719" alt="Êà™Â±è2021-06-02 ‰∏ãÂçà12 02 34" src="https://user-images.githubusercontent.com/65494921/120422433-ce646d00-c39a-11eb-9888-c255abe28d3c.png">

<img width="676" alt="Êà™Â±è2021-06-02 ‰∏ãÂçà12 02 45" src="https://user-images.githubusercontent.com/65494921/120422439-d4f2e480-c39a-11eb-94c4-421442ec3e66.png">


